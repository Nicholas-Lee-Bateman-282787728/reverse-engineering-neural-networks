{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import renn\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.experimental import stax, optimizers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = '/Users/nirum/code/reverse-engineering-neural-networks/vocab/ag_news.vocab'\n",
    "\n",
    "# Load data\n",
    "train_dset = renn.data.ag_news('train', vocab_file, sequence_length=50)\n",
    "test_dset = renn.data.ag_news('test', vocab_file, sequence_length=50)\n",
    "\n",
    "# Load vocab\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = f.readlines()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "example = next(iter(train_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SequenceSum():\n",
    "    def init_fun(_, input_shape):\n",
    "        return (input_shape[0], input_shape[2]), ()\n",
    "    def apply_fun(_, inputs, **kwargs):\n",
    "        return jnp.sum(inputs, axis=1)\n",
    "    return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 32\n",
    "num_classes = 4\n",
    "input_shape = (-1, 50)\n",
    "l2_pen = 1e-3\n",
    "\n",
    "# Linear model\n",
    "init_fun, apply_fun = stax.serial(\n",
    "    renn.embedding(vocab_size, emb_size),\n",
    "    SequenceSum(),\n",
    "    stax.Dense(num_classes),\n",
    "    stax.LogSoftmax,\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(0)\n",
    "output_shape, initial_params = init_fun(key, input_shape)\n",
    "\n",
    "# Hack to set the embedding for 0 to 0\n",
    "emb = params[0]\n",
    "new_emb = np.array(emb)\n",
    "new_emb[0] = np.zeros(emb_size)\n",
    "initial_params = [jnp.array(new_emb), *params[1:]]\n",
    "\n",
    "# Loss\n",
    "def xent(params, batch):\n",
    "    logits = apply_fun(params, batch['inputs'])\n",
    "    targets = renn.one_hot(batch['labels'], num_classes)\n",
    "    data_loss = -jnp.mean(jnp.sum(targets * logits, axis=1))\n",
    "    reg_loss = l2_pen * renn.norm(params)\n",
    "    return data_loss + reg_loss\n",
    "\n",
    "f_df = jax.value_and_grad(xent)\n",
    "\n",
    "# Accuracy\n",
    "@jax.jit\n",
    "def accuracy(params, batch):\n",
    "    logits = apply_fun(params, batch['inputs'])\n",
    "    predictions = jnp.argmax(logits, axis=1)\n",
    "    return jnp.mean(predictions == batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "learning_rate = optimizers.exponential_decay(2e-3, 1000, 0.8)\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "\n",
    "state = init_opt(initial_params)\n",
    "losses = []\n",
    "\n",
    "@jax.jit\n",
    "def step(k, opt_state, batch):\n",
    "    params = get_params(opt_state)\n",
    "    loss, gradients = f_df(params, batch)\n",
    "    new_state = update_opt(k, gradients, opt_state)\n",
    "    return new_state, loss\n",
    "\n",
    "def test_acc(params):\n",
    "    return jnp.array([accuracy(params, batch) for batch in tfds.as_numpy(test_dset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "== Epoch #0\n",
      "== Test accuracy: 23.79%\n",
      "=====================================\n",
      "[step 0]\tLoss: nan\n",
      "[step 100]\tLoss: 0.7176\n",
      "[step 200]\tLoss: 0.3863\n",
      "[step 300]\tLoss: 0.3486\n",
      "[step 400]\tLoss: 0.3514\n",
      "[step 500]\tLoss: 0.3505\n",
      "[step 600]\tLoss: 0.3293\n",
      "[step 700]\tLoss: 0.3424\n",
      "[step 800]\tLoss: 0.3518\n",
      "[step 900]\tLoss: 0.3418\n",
      "[step 1000]\tLoss: 0.3438\n",
      "[step 1100]\tLoss: 0.3183\n",
      "[step 1200]\tLoss: 0.3243\n",
      "[step 1300]\tLoss: 0.3280\n",
      "[step 1400]\tLoss: 0.3340\n",
      "[step 1500]\tLoss: 0.3169\n",
      "[step 1600]\tLoss: 0.3031\n",
      "[step 1700]\tLoss: 0.3306\n",
      "=====================================\n",
      "== Epoch #1\n",
      "== Test accuracy: 90.74%\n",
      "=====================================\n",
      "[step 1800]\tLoss: 0.2796\n",
      "[step 1900]\tLoss: 0.2495\n",
      "[step 2000]\tLoss: 0.2194\n",
      "[step 2100]\tLoss: 0.2161\n",
      "[step 2200]\tLoss: 0.2212\n",
      "[step 2300]\tLoss: 0.2160\n",
      "[step 2400]\tLoss: 0.2262\n",
      "[step 2500]\tLoss: 0.2259\n",
      "[step 2600]\tLoss: 0.2244\n",
      "[step 2700]\tLoss: 0.2315\n",
      "[step 2800]\tLoss: 0.2324\n",
      "[step 2900]\tLoss: 0.2202\n",
      "[step 3000]\tLoss: 0.2260\n",
      "[step 3100]\tLoss: 0.2385\n",
      "[step 3200]\tLoss: 0.2200\n",
      "[step 3300]\tLoss: 0.2230\n",
      "[step 3400]\tLoss: 0.2283\n",
      "=====================================\n",
      "== Epoch #2\n",
      "== Test accuracy: 90.31%\n",
      "=====================================\n",
      "[step 3500]\tLoss: 0.2346\n",
      "[step 3600]\tLoss: 0.2000\n",
      "[step 3700]\tLoss: 0.1901\n",
      "[step 3800]\tLoss: 0.1839\n",
      "[step 3900]\tLoss: 0.1764\n",
      "[step 4000]\tLoss: 0.1781\n",
      "[step 4100]\tLoss: 0.1731\n",
      "[step 4200]\tLoss: 0.1827\n",
      "[step 4300]\tLoss: 0.1848\n",
      "[step 4400]\tLoss: 0.1850\n",
      "[step 4500]\tLoss: 0.1809\n",
      "[step 4600]\tLoss: 0.1754\n",
      "[step 4700]\tLoss: 0.1732\n",
      "[step 4800]\tLoss: 0.1857\n",
      "[step 4900]\tLoss: 0.1860\n",
      "[step 5000]\tLoss: 0.1700\n",
      "[step 5100]\tLoss: 0.1741\n",
      "[step 5200]\tLoss: 0.1887\n",
      "=====================================\n",
      "== Epoch #2\n",
      "== Test accuracy: 89.89%\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    print('=====================================')\n",
    "    print(f'== Epoch #{epoch}')\n",
    "    p = get_params(state)\n",
    "    acc = np.mean(test_acc(p))\n",
    "    print(f'== Test accuracy: {100. * acc:0.2f}%')\n",
    "    print('=====================================')\n",
    "    \n",
    "    for batch in tfds.as_numpy(train_dset):\n",
    "        k = len(losses)\n",
    "        state, loss = step(k, state, batch)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if k % 100 == 0:\n",
    "            p = get_params(state)\n",
    "            print(f'[step {k}]\\tLoss: {np.mean(losses[k-100:k]):0.4f}', flush=True)\n",
    "\n",
    "print('=====================================')\n",
    "print(f'== Epoch #{epoch}')\n",
    "p = get_params(state)\n",
    "acc = np.mean(test_acc(p))\n",
    "print(f'== Test accuracy: {100. * acc:0.2f}%')\n",
    "print('=====================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
